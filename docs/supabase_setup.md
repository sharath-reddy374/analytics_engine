# Supabase setup and table creation (end-to-end)

This project now supports persisting pipeline runs, events, features, decisions, and email attempts in Supabase (Postgres). Follow these steps to configure Supabase, create all tables, and run the pipeline for a user.

Note: You mentioned you added `supabaseUrl` and `supabaseKey` to your `.env`. Those are mapped to `settings.SUPABASE_URL` and `settings.SUPABASE_ANON_KEY` for potential REST usage later. For the database engine and migrations, you MUST also set a Postgres connection string via `DATABASE_URL` (or `SUPABASE_DB_URL`).

---

1) Get your Supabase Postgres connection string

- In Supabase Dashboard:
  - Settings → Database → Connection string
  - Choose “URI” or “SQLAlchemy” format
- It usually looks like:
  postgresql://postgres:YOUR-PASSWORD@db.YOUR-PROJECT-REF.supabase.co:5432/postgres?sslmode=require
- Add one of the following to your `.env` (the code supports all of these, in priority order):
  - DATABASE_URL=postgresql://...
  - or SUPABASE_DB_URL=postgresql://...
  - Also keep your existing REST keys if you want:
    - supabaseUrl=https://YOUR-PROJECT-REF.supabase.co
    - supabaseKey=YOUR-ANON-KEY

2) Install Python dependencies

- Ensure you’re in project root, then:
  pip install -r requirements.txt

3) Verify DB connectivity (optional quick check)

- The engine initializes automatically from `config/settings.py` when `DATABASE_URL` (or alias) is set.
- Run:
  make db-check
- Expected output: “DB engine OK”
  - If you see “DB engine disabled…”, your `DATABASE_URL` is missing or invalid.

4) Create tables in Supabase

Option A: Supabase SQL Editor (100% hosted; easiest)
- Open Supabase Dashboard → SQL Editor
- Open the file `database/supabase_schema.sql` locally and copy its entire content
- Paste into SQL Editor and run it
- You should see these objects created:
  - Types: run_status, email_status
  - Tables: users, runs, events, features, decisions, email_templates, email_attempts, automation_triggers, email_suppression
  - View: latest_features
- Verify under Database → Tables that they exist

Option B: Local apply script (runs from your machine)
- Ensure `DATABASE_URL` is set in `.env`
- Run:
  make apply-sql SCHEMA=database/supabase_schema.sql
  - Internally calls: python scripts/apply_sql.py database/supabase_schema.sql
- Check Supabase UI → Tables to confirm

5) Run the pipeline for one user and persist everything

- We instrumented `scripts/process_single_user.py` to:
  - upsert `users` by email
  - create a `runs` row on start and mark success/failed at end
  - append `events` at each step
  - persist computed `features`
  - persist rule `decisions`
  - idempotently queue `email_attempts` for the selected template(s) with unique key (user_id:template_key:stage)
- Run (dry-run email sending unless you wired a real EmailService):
  python scripts/process_single_user.py user@example.com --verbose
- Or via Makefile:
  make process-user EMAIL=user@example.com

6) Verify data in Supabase

- Useful queries (in SQL Editor or psql):
  -- Find user
  SELECT id, email, created_at FROM public.users WHERE email ILIKE 'user@example.com';

  -- Runs for that user (replace :uid with the returned UUID)
  SELECT * FROM public.runs WHERE user_id = :uid ORDER BY started_at DESC LIMIT 5;

  -- Events generated by the last run
  SELECT * FROM public.events WHERE run_id = :rid ORDER BY occurred_at ASC;

  -- Features saved for that run
  SELECT name, value, computed_at FROM public.features WHERE user_id = :uid AND run_id = :rid;

  -- Decisions recorded
  SELECT rule, decision, rationale, decided_at FROM public.decisions WHERE user_id = :uid AND run_id = :rid;

  -- Email attempts (idempotent)
  SELECT template_key, stage, status, reason, unique_key, created_at
  FROM public.email_attempts
  WHERE user_id = :uid
  ORDER BY created_at DESC;

7) Idempotency and “don’t send the same email twice”

- Each queued email attempt for a user uses a unique key:
  unique_key = "{user_id}:{template_key}:{stage}"
- `email_attempts.unique_key` has a UNIQUE constraint. If the same attempt is enqueued twice, the second insert is ignored.
- `scripts/process_single_user.py` pre-enqueues attempts with `stage='initial'`. If you later add follow-up logic, use `stage='followup_1'`, etc.

8) Follow-ups and future triggers

- A table `automation_triggers` is provisioned for scheduling logic. A simple pattern:
  - Your scheduler (e.g., cron, Airflow, or Supabase cron + Edge functions) finds triggers due (active = true and now() >= next_fire_at)
  - Your worker evaluates conditions (e.g., no open/click events, last email attempt older than X days) and enqueues next `email_attempts` row with the next stage
  - Update or deactivate the trigger and log an `events` row
- The included `scripts/daily_pipeline.py` remains DRY RUN by default and shows how to structure daily compute and decisions. You can extend it to evaluate triggers and insert into `email_attempts`.

9) Configuration recap

- .env must include:
  DATABASE_URL=postgresql://postgres:YOUR_PASSWORD@db.YOUR-PROJECT-REF.supabase.co:5432/postgres?sslmode=require
  # Optional (already added by you; used if/when we add REST access):
  supabaseUrl=https://YOUR-PROJECT-REF.supabase.co
  supabaseKey=YOUR-ANON-KEY
- Optional flags you might care about in `config/settings.py`:
  - EMAIL_SIMULATION_MODE=true
  - MAX_EMAILS_PER_DAY, MAX_EMAILS_PER_WEEK, EMAIL_QUIET_HOURS_START/END

10) Troubleshooting

- “DB engine disabled”:
  - Ensure `.env` is loaded and `DATABASE_URL` is correctly set
  - Run `python -c "from config.settings import settings; print(settings.DATABASE_URL)"` to confirm the value
- “permission denied to create type” (very rare in Supabase):
  - Run the schema in the SQL Editor as the default `postgres` role (the GUI does this)
- “relation already exists”:
  - The schema uses IF NOT EXISTS where possible. If you partially created objects before, you can safely re-run.

What changed in the codebase

- Added `database/supabase_schema.sql` with the full relational schema
- Added `database/postgres.py` (helpers for inserts/updates)
- Added `services/event_logger.py` (thin wrapper used by the pipeline)
- Instrumented `scripts/process_single_user.py` to log runs/events/features/decisions and queue email attempts
- Added `scripts/apply_sql.py` (convenient local migration tool)
- Added Make targets:
  - make db-check
  - make apply-sql SCHEMA=database/supabase_schema.sql
- Updated `config/settings.py` to accept `supabaseUrl`, `supabaseKey`, and alias `SUPABASE_DB_URL` to `DATABASE_URL`

Next steps (optional)

- Wire an email delivery worker to pick up `email_attempts` with status='queued', send, and set status='sent' with `event_logger.mark_email_sent(...)`
- Extend `scripts/daily_pipeline.py` to read due triggers and enqueue follow-ups (`stage='followup_1'`, etc.)
- Add suppression logic in `email_suppression` for opt-outs (by template or global)
